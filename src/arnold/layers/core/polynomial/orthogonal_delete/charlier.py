import tensorflow as tf
import numpy as np

from ..poly_base import PolynomialBase

tfk = tf.keras
tfkl = tfk.layers


@tfk.utils.register_keras_serializable(package="arnold", name="Charlier")
class Charlier(PolynomialBase):
    """
    Kolmogorov-Arnold Network layer using the Charlier polynomials.

    The Charlier polynomials are generated by the three-term recurrence relation:

    * :math:`C_{-1; a}(x) = 0`
    * :math:`C_{0; a}(x) = 1`
    * :math:`x * C_{n}(x; a) = C_{n+1}(x; a) + (n + a) * C_{n}(x; a) + a * n * C_{n-1}(x; a)` 

    for :math:`a>0`.

    See: https://arxiv.org/pdf/1901.06041, eq. 1.4
    """

    def __init__(
            self, 
            *args,
            a_init: float | None = None, a_trainable=True, 
            **kwargs):
        
        super().__init__(*args, **kwargs)
        """
        :param input_dim: This layers input size
        :type input_dim: int

        :param output_dim: This layers output size
        :type output_dim: int

        :param degree: The maximum degree of the polynomial basis element (default is 3).
        :type degree: int

        :param decompose_weights: Whether or not to represent the polynomial_coefficients weights tensor as a learnable Tucker decomposition. Default to False.
        :type decompose_weights: bool

        :param core_ranks: A 3-tuple of non-zero, positive integers giving the ranks of the Tucker decomposition core tensor. Ignored if `decompose_weights` is False; defaults to None.
        :type core_ranks: None | Tuple[int, int, int]

        :param tanh_x: Flag indicating whether to normalize any input to [-1, 1] using tanh before further processing.
        :type tanh_x: bool

        :param a_init: Initial value for the a parameter of the Charlier polynomials. Defaults to None (a initialized to RandomNormal).
        :type a_init: float | None = None

        :param a_trainable: Flag indicating whether a is a trainable parameter. Defaults to True
        :type a_trainable: bool
        """ 
        self.a_init = a_init
        self.a_trainable = a_trainable

        self.a = self.add_weight(
            initializer=tfk.initializers.Constant(value=self.a_init) if self.a_init else tfk.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),
            name='a',
            trainable=self.a_trainable,
            regularizer=None,
            constraint=None,
        )

    @tf.function
    def pseudo_vandermonde(self, x):

        # :math:`a > 0`
        a = tf.exp(self.a)
        
        # :math:`C_{-1; a}(x) = 0`
        # :math:`C_{0; a}(x) = 1`
        charlier_basis = [
            tf.ones_like(x)
        ]

        if self.degree > 0:
            # :math`C_{1}(x; a) = (x - (n + a)) * C_{0}(x; a) - a*n*C_{-1}(x; a)`
            charlier_basis.append(
                (x - (1 + a))  
            )
        
        for n in range(2, self.degree + 1):
            # :math:`x * C_{n}(x; a) = C_{n+1}(x; a) + (n + a) * C_{n}(x; a) + a * n * C_{n-1}(x; a)` 
            charlier_basis.append(
                (x - (n + a)) * charlier_basis[n-1] - a * n * charlier_basis[n-2]
            )

        return tf.stack(charlier_basis, axis=-1)
    
    def get_config(self):
        config = super().get_config()
        config.update({
            "a_init": self.a_init,
            "a_trainable": self.a_trainable,
        })
        return config
