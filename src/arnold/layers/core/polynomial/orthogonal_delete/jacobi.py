import tensorflow as tf
import numpy as np

from ..poly_base import PolynomialBase

tfk = tf.keras
tfkl = tfk.layers


@tfk.utils.register_keras_serializable(package="arnold", name="Jacobi")
class Jacobi(PolynomialBase):
    r"""
    Kolmogorov-Arnold Network layer using Jacobi polynomials.

    The Jacobi polynomials are generated by the three-term recurrence relation:

    * :math:`J^{\alpha, \beta}_{0}(x) = 1`
    * :math:`J^{\alpha, \beta}_{1}(x) = \frac{1}{2} * (\alpha + \beta + 2) * x + \frac{1}{2} * (\alpha - \beta)`
    * :math:`J^{\alpha, \beta}_{n+1}(x) = (A^{\alpha, \beta}_{n} * x - B^{\alpha, \beta}_{n}) * J^{\alpha, \beta}_{n}(x) - C^{\alpha, \beta}_{n} * J^{\alpha, \beta}_{n-1}(x)` when n >= 1

    with 

    * :math:`A^{\alpha, \beta}_{n} = \frac{(2n + \alpha + \beta +1) * (2n + \alpha + \beta + 2)}{2(n+1) * (n + \alpha + \beta + 1)}`
    * :math:`B^{\alpha, \beta}_{n} = \frac{(\beta^{2} - \alpha^{2})(2n + \alpha + \beta +1)}{2(n+1) * (n + \alpha + \beta + 1)(2n + \alpha + \beta)}`
    * :math:`C^{\alpha, \beta}_{n} = \frac{(n + \alpha)(n + \beta)(2n + \alpha + \beta + 2)}{(n+1) * (n + \alpha + \beta + 1)(2n + \alpha + \beta)}`

    Special cases of the Jacobi polynomials are: 
    * the Legendre polynomials (when alpha=beta=0); 
    * the Chebyshev polynomials of the first kind (when alpha=beta=-1/2); 
    * the Chebyshev polynomials of the second kind (when alpha=beta==1/2); 
    * the Gegenbauer polynomials (when alpha=beta)


    TODO:     Jacobi polynomials in hypergeometric representation
    :math:`J^{\alpha, \beta}_{n}(x) = \binom{n + \alpha}{n} * _2F_1(-n, n + \alpha + \beta + 1; \alpha + 1; \frac{1-x}{2}d)
    """

    def __init__(
            self, 
            *args,
            alpha_init: float | None = None, 
            alpha_trainable=True, 
            beta_init: float | None = None, 
            beta_trainable=True,
            **kwargs):
        """
        :param input_dim: This layers input size
        :type input_dim: int

        :param output_dim: This layers output size
        :type output_dim: int

        :param degree: The maximum degree of the polynomial basis element (default is 3).
        :type degree: int

        :param decompose_weights: Whether or not to represent the polynomial_coefficients weights tensor as a learnable Tucker decomposition. Default to False.
        :type decompose_weights: bool

        :param core_ranks: A 3-tuple of non-zero, positive integers giving the ranks of the Tucker decomposition core tensor. Ignored if `decompose_weights` is False; defaults to None.
        :type core_ranks: None | Tuple[int, int, int]

        :param tanh_x: Flag indicating whether to normalize any input to [-1, 1] using tanh before further processing.
        :type tanh_x: bool

        :param alpha_init: Initial value for the alpha parameter of the Jacobi polynomials. Defaults to None (a initialized to RandomNormal).
        :type alpha_init: float | None = None

        :param alpha_trainable: Flag indicating whether alpha is a trainable parameter. Defaults to True
        :type alpha_trainable: bool

        :param beta_init: Initial value for the beta parameter of the Jacobi polynomials. Defaults to None (a initialized to RandomNormal).
        :type beta_init: float | None = None

        :param beta_trainable: Flag indicating whether beta is a trainable parameter. Defaults to True
        :type beta_trainable: bool
        """ 
        super().__init__(*args, **kwargs)

        self.alpha_init = alpha_init
        self.alpha_trainable = alpha_trainable
        self.beta_init = beta_init
        self.beta_trainable = beta_trainable

        self.alpha = self.add_weight(
            initializer=tfk.initializers.Constant(value=self.alpha_init) if self.alpha_init else tfk.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),
            name='alpha',
            trainable=self.alpha_trainable
        )

        self.beta = self.add_weight(
            initializer=tfk.initializers.Constant(value=self.beta_init) if self.beta_init else tfk.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),
            name='beta',
            trainable=self.beta_trainable
        )

    @tf.function
    def pseudo_vandermonde(self, x):
        # See: http://lsec.cc.ac.cn/~hyu/teaching/shonm2013/STWchap3.2p.pdf (section 3.2.1.3)
        # :math:`J^{\alpha, \beta}_{0}(x) = 1`
        jacobi_basis = [ tf.ones_like(x) ]

        if self.degree > 0:
            # :math:`J^{\alpha, \beta}_{1}(x) = \frac{1}{2} * (\alpha + \beta + 2) * x + \frac{1}{2} * (\alpha - \beta)`
            jacobi_basis.append(
                (0.5 * (self.alpha - self.beta) + (self.alpha + self.beta + 2) * x / 2)
            )

        for n in range(2, self.degree + 1):
            # :math:`A^{\alpha, \beta}_{n} = \frac{(2n + \alpha + \beta +1) * (2n + \alpha + \beta + 2)} {2(n+1) * (n + \alpha + \beta + 1)}`
            A_n = tf.divide(
                (2*n + self.alpha + self.beta + 1) * (2*n + self.alpha + self.beta + 2),
                2 * (n + 1) * (n + self.alpha + self.beta + 1)
            )
            # :math:`B^{\alpha, \beta}_{n} = \frac{(\beta^{2} - \alpha^{2})(2n + \alpha + \beta +1)}{2(n+1) * (n + \alpha + \beta + 1)(2n + \alpha + \beta)}`
            B_n = tf.divide(
                (self.beta**2 - self.alpha**2) * (2 * n + self.alpha + self.beta + 1),
                2 * (n +1) * (n + self.alpha + self.beta + 1) * (2 * n + self.alpha + self.beta)
            )
            # :math:`C^{\alpha, \beta}_{n} = \frac{(n + \alpha)(n + \beta)(2n + \alpha + \beta + 2)}{(n+1) * (n + \alpha + \beta + 1)(2n + \alpha + \beta)}``
            C_n = tf.divide(
                (n + self.alpha) * (n + self.beta) * (2 * n + self.alpha + self.beta + 2),
                (n+1) * (n + self.alpha + self.beta + 1) * (2 * n + self.alpha + self.beta)
            )

            # :math:`J^{\alpha, \beta}_{n+1}(x) = (A^{\alpha, \beta}_{n} * x - B^{\alpha, \beta}_{n}) * J^{\alpha, \beta}_{n}(x) - C^{\alpha, \beta}_{n} * J^{\alpha, \beta}_{n-1}(x)` when n >= 1
            jacobi_basis.append(
                A_n * x - B_n * jacobi_basis[n-1] - C_n * jacobi_basis[n-2]
            )

        return tf.stack(jacobi_basis, axis=-1)

    def get_config(self):
        config = super().get_config()
        config.update({
            "alpha_init": self.alpha_init,
            "alpha_trainable": self.alpha_trainable,
            "beta_init": self.beta_init,
            "beta_trainable": self.beta_trainable,
        })
        return config
