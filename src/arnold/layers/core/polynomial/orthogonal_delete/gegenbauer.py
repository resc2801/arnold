import tensorflow as tf
import numpy as np
from typing import Tuple

from ..poly_base import PolynomialBase

tfk = tf.keras
tfkl = tfk.layers


@tfk.utils.register_keras_serializable(package="arnold", name="Gegenbauer")
class Gegenbauer(PolynomialBase):
    r"""
    Kolmogorov-Arnold Network layer using Gegenbauer polynomials.

    The Gegenbauer polynomials are generated by the three-term recurrence relation:

    * :math:`C^{\alpha}_{0}(x) = 1`
    * :math:`C^{\alpha}_{1}(x) = 2 * \alpha * x`
    * :math:`C^{\alpha}_{n+1}(x) = \frac{(2 * (n + \alpha) * x * C^{\alpha}_{n}(x)) - ((n + 2 * \alpha - 1) * C^{\alpha}_{n - 1}(x))}{n + 1}` when n >= 1

    See also: https://en.wikipedia.org/wiki/Gegenbauer_polynomials#Characterizations

    They generalize Legendre polynomials and Chebyshev polynomials, and are special cases of Jacobi polynomials.
    """

    def __init__(
            self, 
            *args,
            alpha_init:float | None = None, 
            alpha_trainable=True,
            **kwargs):
        """
        :param input_dim: This layers input size
        :type input_dim: int

        :param output_dim: This layers output size
        :type output_dim: int

        :param degree: The maximum degree of the polynomial basis element (default is 3).
        :type degree: int

        :param decompose_weights: Whether or not to represent the polynomial_coefficients weights tensor as a learnable Tucker decomposition. Default to False.
        :type decompose_weights: bool

        :param core_ranks: A 3-tuple of non-zero, positive integers giving the ranks of the Tucker decomposition core tensor. Ignored if `decompose_weights` is False; defaults to None.
        :type core_ranks: None | Tuple[int, int, int]

        :param tanh_x: Flag indicating whether to normalize any input to [-1, 1] using tanh before further processing.
        :type tanh_x: bool

        :param alpha_init: Initial value for the alpha parameter of the Gegenbauer polynomials. Defaults to None (alpha with RandomNormal initialization).
        :type alpha_init: float | None = None

        :param alpha_trainable: Flag indicating whether alpha is a trainable parameter. Defaults to True
        :type alpha_trainable: bool
        """
        super().__init__(*args, **kwargs)

        self.alpha_init = alpha_init
        self.alpha_trainable = alpha_trainable

        self.alpha = self.add_weight(
            initializer=tfk.initializers.Constant(value=self.alpha_init) if self.alpha_init else tfk.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),
            name='alpha',
            trainable=self.alpha_trainable
        )

    @tf.function
    def pseudo_vandermonde(self, x):
        # :math:`C^{\alpha}_{0}(x) = 1`
        gegenbauer_basis = [ tf.ones_like(x) ]

        if self.degree > 0:
            # :math:`C^{\alpha}_{1}(x) = 2 * \alpha x`
            gegenbauer_basis.append(
                2 * self.alpha * x
            )

        for n in range(2, self.degree + 1):
            # :math:`C^{\alpha}_{n+1}(x) = \frac{(2 * (n + \alpha) * x * C^{\alpha}_{n}(x)) - ((n + 2 * \alpha - 1) * C^{\alpha}_{n - 1}(x))}{n + 1}` when n >= 1
            gegenbauer_basis.append(
                ((2 * ((n - 1) + self.alpha) * x * gegenbauer_basis[n-1]) - (((n - 1) + 2 * self.alpha - 1) * gegenbauer_basis[n-2])) / n
            )
        
        return tf.stack(gegenbauer_basis, axis=-1)

    def get_config(self):
        config = super().get_config()
        config.update({
            "alpha_init": self.alpha_init,
            "alpha_trainable": self.alpha_trainable,
        })
        return config
